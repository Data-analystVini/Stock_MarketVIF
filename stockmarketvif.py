# -*- coding: utf-8 -*-
"""stockmarketvif.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ihffbZTyzcxBSU01BhvZ6RC2iWnxywaV
"""

import pandas as pd
import numpy as np

stockmarket_data=pd.read_excel('/content/drive/MyDrive/stockmarket_data.xlsx',parse_dates=["Date"])
stockmarket_data

print(stockmarket_data.dtypes)

print(stockmarket_data.isnull().sum())

# Example: Filling missing values with the mean
stockmarket_data.fillna(stockmarket_data.mean(), inplace=True)

# Select only numeric columns
numeric_data = stockmarket_data.select_dtypes(include=['number'])

# Assuming 'Close*' is your target variable
X = numeric_data.drop(columns='Close*')
Y=Y=stockmarket_data['Close*']

print(X)

print(Y)

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=1)

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Assuming 'Close*' is your target column
X = stockmarket_data.drop(columns=['Close*','Date'])  # Drop the target column from features
Y = stockmarket_data['Close*']  # Target variable

# Split the data into training and testing sets BEFORE applying PCA
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA on the training data (decide on n_components or use variance explained ratio)
pca = PCA(n_components=0.90)  # Keep 90% variance
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Train the model on PCA-transformed training data
model = LinearRegression()
model.fit(X_train_pca, Y_train)

# Make predictions
Y_pred = model.predict(X_test_pca)

# Calculate MSE and RMSE
mse = mean_squared_error(Y_test, Y_pred)
rmse = np.sqrt(mse)

print(f"MSE: {mse}")
print(f"RMSE: {rmse}")

"""**PLOTTING A GRAPH BASED ON PCA**"""

# Plot Predictions vs Actual values
plt.figure(figsize=(8, 6))
plt.scatter(Y_test,Y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(Y_test), max(Y_test)], [min(Y_test), max(Y_test)], color='red', label='Ideal Fit Line')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs Actual Values')
plt.legend()
plt.grid(True)
plt.show()

"""**Calculate VIF**

**n machine learning, the variance inflation factor (VIF) is used to identify and address multicollinearity in regression models. Multicollinearity occurs when two or more variables in a regression analysis have a linear relationship with each other. VIF is a statistical measure that helps quantify the extent of multicollinearity in a dataset**
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Initialize a DataFrame to hold VIF values
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns

# Calculate VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF results
print(vif_data)

# Assuming you have calculated the VIF and stored it in `vif_data`
# Drop features with high VIF values (threshold can be 5 or 10)
high_vif_features = vif_data[vif_data["VIF"] > 5]["feature"]
X_reduced = X.drop(columns=high_vif_features)

from sklearn.model_selection import train_test_split

# Assuming `Close*` is your target column
Y = stockmarket_data['Close*']
X_train, X_test, Y_train, Y_test = train_test_split(X_reduced, Y, test_size=0.2, random_state=42)

# Example: Using scaled data in Linear Regression
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train_scaled, Y_train)

# Predictions
Y_pred = model.predict(X_test_scaled)

from sklearn.metrics import mean_squared_error
mse=mean_squared_error(Y_test,Y_pred)
print(mse)

rmse=np.sqrt(mse)
print("rmse",{rmse})

"""**PLOTTING GRAPH BASED ON VIF**"""

# Plot Predictions vs Actual values
plt.figure(figsize=(8, 6))
plt.scatter(Y_test, Y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(Y_test), max(Y_test)], [min(Y_test), max(Y_test)], color='red', label='Ideal Fit Line')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs Actual Values')
plt.legend()
plt.grid(True)
plt.show()

"""PCA
 PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space. **bold text**
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Assuming 'Close*' is your target variable
#X = stockmarket_data.drop(columns='Close*')  # Drop target column
#PCA is sensitive to the scale of the data, so itâ€™s important to standardize (normalize)
#the features before applying PCA. We use StandardScaler from sklearn.preprocessing to standardize the dataset.

# Standardize the data (PCA is sensitive to the scale of the data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=None)  # Keep all components initially
X_pca = pca.fit_transform(X_scaled)

# Explained variance for each principal component
explained_variance = pca.explained_variance_ratio_

# Create a DataFrame to see the explained variance by each component
pca_df = pd.DataFrame({
    'Principal Component': [f"PC{i+1}" for i in range(len(explained_variance))],
    'Explained Variance': explained_variance
})

print(pca_df)

# Plot the explained variance
plt.figure(figsize=(8,6))
plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--', color='b')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by Principal Components')
plt.grid(True)
plt.show()

# Reduce to the number of components that explain 90% of the variance
pca_90 = PCA(n_components=0.90)
X_pca_90 = pca_90.fit_transform(X_scaled)

print(f"Reduced to {pca_90.n_components_} components that explain 90% of the variance")